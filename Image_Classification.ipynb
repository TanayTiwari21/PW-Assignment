{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Convolutional Neural Network (CNN), and how does it differ from\n",
        "traditional fully connected neural networks in terms of architecture and performance on\n",
        "image data?\n",
        "\n",
        "Ans:-\n",
        "\n",
        "Convolutional Neural Network (CNN):\n",
        "\n",
        "A Convolutional Neural Network (CNN) is a type of deep learning model designed specifically for working with image and spatial data.\n",
        "It uses specialized layers such as convolution, pooling, and fully connected layers to automatically and adaptively learn spatial hierarchies of features from input data (like edges, textures, shapes, objects).\n",
        "\n",
        "Key Differences Between CNN and Traditional Fully Connected Neural Networks (FNNs):\n",
        "Aspect\tFully Connected Neural Network (FNN)\tConvolutional Neural Network (CNN)\n",
        "Architecture\tEvery neuron in one layer is connected to every neuron in the next layer.\tUses convolutional layers where filters (kernels) slide over the image to detect local features (edges, patterns).\n",
        "Input Representation\tImages are flattened into 1D vectors (e.g., a 28Ã—28 image â†’ 784 inputs). Spatial/positional information is lost.\tImages are kept in 2D/3D (height Ã— width Ã— channels). Preserves spatial relationships like â€œnearby pixels.â€\n",
        "Parameter Count\tVery large, because every neuron is connected to all inputs (e.g., 1M+ parameters for medium images).\tMuch fewer parameters due to weight sharing in convolution filters (e.g., a 3Ã—3 filter has only 9 weights regardless of image size).\n",
        "Feature Learning\tDoesnâ€™t inherently capture local patterns; needs manual feature engineering or very large data.\tAutomatically learns hierarchical features:\n",
        "- Early layers â†’ edges & textures\n",
        "- Middle layers â†’ shapes & parts\n",
        "- Deeper layers â†’ objects.\n",
        "Performance on Images\tPoorer, prone to overfitting, requires more training data and computation.\tMuch better, state-of-the-art in computer vision tasks (image classification, detection, segmentation).\n",
        "Scalability\tHard to scale for large images (explodes parameter size).\tScales well because convolution reuses filters across the whole image.\n",
        "Why CNNs Perform Better on Images:\n",
        "\n",
        "Local Connectivity: CNN focuses on small regions of the image at a time.\n",
        "\n",
        "Weight Sharing: The same filter is applied across the entire image, reducing parameters and computation.\n",
        "\n",
        "Translation Invariance: CNN can recognize features regardless of their position in the image.\n",
        "\n",
        "Hierarchical Learning: Builds from low-level features (edges) to high-level features (faces/objects)."
      ],
      "metadata": {
        "id": "je-9bztpq24O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Discuss the architecture of LeNet-5 and explain how it laid the foundation\n",
        "for modern deep learning models in computer vision. Include references to its original\n",
        "research paper.\n",
        "\n",
        "Ans:-\n",
        "\n",
        "LeNet-5 Architecture\n",
        "\n",
        "LeNet-5, proposed by Yann LeCun et al. in 1998 (paper: Gradient-Based Learning Applied to Document Recognition), is one of the earliest and most influential Convolutional Neural Networks (CNNs). It was designed for handwritten digit recognition (e.g., MNIST dataset).\n",
        "\n",
        "Architecture Overview (7 layers excluding input)\n",
        "\n",
        "Input:\n",
        "\n",
        "32\n",
        "Ã—\n",
        "32\n",
        "32Ã—32 grayscale image. (MNIST digits are\n",
        "28\n",
        "Ã—\n",
        "28\n",
        "28Ã—28, padded to\n",
        "32\n",
        "Ã—\n",
        "32\n",
        "32Ã—32).\n",
        "\n",
        "Layer-by-Layer Structure:\n",
        "\n",
        "Input Layer:\n",
        "\n",
        "32\n",
        "Ã—\n",
        "32\n",
        "32Ã—32 pixel image.\n",
        "\n",
        "C1 â€“ Convolution Layer:\n",
        "\n",
        "6 feature maps, each of size\n",
        "28\n",
        "Ã—\n",
        "28\n",
        "28Ã—28.\n",
        "\n",
        "Kernel size:\n",
        "5\n",
        "Ã—\n",
        "5\n",
        "5Ã—5.\n",
        "\n",
        "Operation: Detects simple patterns like edges or blobs.\n",
        "\n",
        "S2 â€“ Subsampling (Average Pooling) Layer:\n",
        "\n",
        "6 feature maps, each\n",
        "14\n",
        "Ã—\n",
        "14\n",
        "14Ã—14.\n",
        "\n",
        "Pooling factor:\n",
        "2\n",
        "Ã—\n",
        "2\n",
        "2Ã—2.\n",
        "\n",
        "Purpose: Downsamples features, reduces computation, and provides translation invariance.\n",
        "\n",
        "C3 â€“ Convolution Layer:\n",
        "\n",
        "16 feature maps, each\n",
        "10\n",
        "Ã—\n",
        "10\n",
        "10Ã—10.\n",
        "\n",
        "Not fully connected to previous maps (sparse connections reduce parameters).\n",
        "\n",
        "Extracts more complex features (shapes, textures).\n",
        "\n",
        "S4 â€“ Subsampling Layer:\n",
        "\n",
        "16 feature maps, each\n",
        "5\n",
        "Ã—\n",
        "5\n",
        "5Ã—5.\n",
        "\n",
        "Again, average pooling with stride 2.\n",
        "\n",
        "C5 â€“ Convolution Layer (Fully Connected):\n",
        "\n",
        "120 feature maps of size\n",
        "1\n",
        "Ã—\n",
        "1\n",
        "1Ã—1.\n",
        "\n",
        "Each neuron connected to all\n",
        "5\n",
        "Ã—\n",
        "5\n",
        "5Ã—5 maps from S4.\n",
        "\n",
        "F6 â€“ Fully Connected Layer:\n",
        "\n",
        "84 neurons.\n",
        "\n",
        "Uses a Gaussian RBF-like activation function.\n",
        "\n",
        "Output Layer:\n",
        "\n",
        "10 neurons (corresponding to digits 0â€“9).\n",
        "\n",
        "Softmax activation used in modern implementations.\n",
        "\n",
        "Original Paper:\n",
        "\n",
        "Yann LeCun, LÃ©on Bottou, Yoshua Bengio, and Patrick Haffner.\n",
        "Gradient-Based Learning Applied to Document Recognition.\n",
        "Proceedings of the IEEE, 86(11):2278â€“2324, November 1998.\n",
        "Link to paper (IEEE)"
      ],
      "metadata": {
        "id": "s7QSNtb7rI6V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: Compare and contrast AlexNet and VGGNet in terms of design principles,\n",
        "number of parameters, and performance. Highlight key innovations and limitations of\n",
        "each.\n",
        "\n",
        "Ans:-\n",
        "\n",
        "AlexNet vs VGGNet\n",
        "\n",
        "Both AlexNet (2012) and VGGNet (2014) were landmark CNN architectures that shaped the progress of computer vision.\n",
        "\n",
        "1. Design Principles\n",
        "\n",
        "AlexNet (Krizhevsky et al., 2012 â€“ ImageNet Winner)\n",
        "\n",
        "Inspired by LeNet but much deeper (8 layers: 5 conv + 3 FC).\n",
        "\n",
        "Introduced ReLU activation (instead of sigmoid/tanh) â†’ faster training.\n",
        "\n",
        "Used Dropout for regularization.\n",
        "\n",
        "Local Response Normalization (LRN) to encourage competition among neurons.\n",
        "\n",
        "First to leverage GPU training for large-scale deep learning.\n",
        "\n",
        "VGGNet (Simonyan & Zisserman, 2014 â€“ ImageNet Runner-up)\n",
        "\n",
        "Emphasized depth with simplicity â†’ only\n",
        "3\n",
        "Ã—\n",
        "3\n",
        "3Ã—3 convolution filters and\n",
        "2\n",
        "Ã—\n",
        "2\n",
        "2Ã—2 pooling.\n",
        "\n",
        "Tested multiple depths (VGG-11, VGG-16, VGG-19).\n",
        "\n",
        "No exotic tricks (like LRN), just stacked small filters to increase non-linearity while keeping parameters manageable.\n",
        "\n",
        "Demonstrated that depth matters â†’ deeper models generalize better.\n",
        "| Model       | Depth     | Parameters   | Notes                                                   |\n",
        "| ----------- | --------- | ------------ | ------------------------------------------------------- |\n",
        "| **AlexNet** | 8 layers  | ~62 million  | Large FC layers dominate parameter count.               |\n",
        "| **VGG-16**  | 16 layers | ~138 million | Very deep; parameters mainly in fully connected layers. |\n",
        "| **VGG-19**  | 19 layers | ~144 million | Even deeper, but marginal accuracy gain.                |\n",
        "\n",
        "3. Performance (ImageNet Classification)\n",
        "\n",
        "AlexNet (2012):\n",
        "\n",
        "Top-5 Error: ~15.3%\n",
        "\n",
        "Huge improvement vs. traditional methods.\n",
        "\n",
        "Sparked the deep learning revolution in computer vision.\n",
        "\n",
        "VGG-16 (2014):\n",
        "\n",
        "Top-5 Error: ~7.3%\n",
        "\n",
        "Nearly 2Ã— better than AlexNet.\n",
        "\n",
        "Became a benchmark for feature extraction in transfer learning.\n",
        "\n",
        "4. Key Innovations\n",
        "\n",
        "AlexNet Innovations:\n",
        "\n",
        "ReLU activation (faster convergence).\n",
        "\n",
        "Dropout (prevented overfitting).\n",
        "\n",
        "Data augmentation (cropping, flipping).\n",
        "\n",
        "GPU acceleration for large-scale deep learning.\n",
        "\n",
        "VGGNet Innovations:\n",
        "\n",
        "Very deep networks (up to 19 layers).\n",
        "\n",
        "Small\n",
        "3\n",
        "Ã—\n",
        "3\n",
        "3Ã—3 filters stacked â†’ equivalent to larger receptive fields but with fewer parameters.\n",
        "\n",
        "Established a uniform architecture principle: repeatable blocks of conv + pool.\n",
        "\n",
        "Became the basis for many modern architectures (ResNet, EfficientNet).\n",
        "\n",
        "5. Limitations\n",
        "\n",
        "AlexNet Limitations:\n",
        "\n",
        "Relatively shallow by todayâ€™s standards.\n",
        "\n",
        "LRN normalization became obsolete.\n",
        "\n",
        "Still had very large fully connected layers â†’ heavy in parameters.\n",
        "\n",
        "VGGNet Limitations:\n",
        "\n",
        "Extremely large parameter count (~138M), making it memory and computation intensive.\n",
        "\n",
        "Training was slow and required lots of GPU resources.\n",
        "\n",
        "Not efficient for deployment (later replaced by ResNet, MobileNet, etc.)."
      ],
      "metadata": {
        "id": "8EXQprYxrapt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is transfer learning in the context of image classification? Explain\n",
        "how it helps in reducing computational costs and improving model performance with\n",
        "limited data.\n",
        "\n",
        "Ans:-\n",
        "\n",
        "What is Transfer Learning?\n",
        "\n",
        "Transfer learning is a machine learning technique where a model trained on a large dataset (like ImageNet with millions of images) is reused (partially or fully) for a new but related task, such as classifying medical images, detecting animals, or recognizing objects in satellite images.\n",
        "\n",
        "Instead of training from scratch, we transfer the learned features (edges, textures, shapes, objects) from the pre-trained model to the new task.\n",
        "\n",
        "How It Works in Image Classification\n",
        "\n",
        "Pretraining Phase:\n",
        "\n",
        "Train a large CNN (e.g., ResNet, VGG, Inception) on a massive dataset like ImageNet (1.2M images, 1000 classes).\n",
        "\n",
        "The model learns general features:\n",
        "\n",
        "Early layers â†’ edges, corners, textures.\n",
        "\n",
        "Middle layers â†’ shapes, patterns.\n",
        "\n",
        "Deeper layers â†’ high-level object features.\n",
        "\n",
        "Transfer Phase:\n",
        "\n",
        "Use this pretrained model for a new, smaller dataset.\n",
        "\n",
        "Two approaches:\n",
        "\n",
        "Feature Extraction: Freeze pretrained layers and only train the final classifier (usually a fully connected + softmax).\n",
        "\n",
        "Fine-tuning: Unfreeze some deeper layers and retrain them along with the classifier to adapt to the new dataset.\n",
        "\n",
        "Why Transfer Learning is Useful\n",
        "1. Reduces Computational Cost\n",
        "\n",
        "Training a CNN from scratch on millions of images requires huge GPU/TPU resources and weeks of training.\n",
        "\n",
        "With transfer learning, you only retrain a small portion of the network â†’ much faster (hours or days).\n",
        "\n",
        "2. Improves Performance with Limited Data\n",
        "\n",
        "Deep CNNs need lots of labeled data to generalize well.\n",
        "\n",
        "Transfer learning leverages knowledge from large datasets to perform well on small datasets (e.g., medical images where data is scarce).\n",
        "\n",
        "Prevents overfitting by using robust pretrained weights instead of randomly initialized ones.\n",
        "\n",
        "3. Leverages Learned Representations\n",
        "\n",
        "Early convolutional filters are universal features (edges, textures) â†’ useful across many tasks.\n",
        "\n",
        "Only task-specific layers need retraining.\n",
        "\n",
        "Example\n",
        "\n",
        "Suppose you want to classify X-ray images into â€œdiseaseâ€ vs â€œhealthyâ€:\n",
        "\n",
        "Training a CNN from scratch â†’ impractical (since labeled medical data is small).\n",
        "\n",
        "Using a pretrained ResNet trained on ImageNet â†’ reuse feature extractor, retrain final layer with X-ray dataset.\n",
        "\n",
        "Result â†’ better accuracy, less training time, lower risk of overfitting."
      ],
      "metadata": {
        "id": "85MjQIB0rteT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Describe the role of residual connections in ResNet architecture. How do\n",
        "they address the vanishing gradient problem in deep CNNs?\n",
        "\n",
        "Ans:-\n",
        "\n",
        "Residual Connections in ResNet\n",
        "\n",
        "ResNet (Residual Network), introduced by He et al. in 2015, solved the problem of training very deep CNNs (hundreds of layers).\n",
        "\n",
        "Residual Connection (Skip Connection)\n",
        "\n",
        "Instead of learning a direct mapping\n",
        "ð»\n",
        "(\n",
        "ð‘¥\n",
        ")\n",
        "H(x), ResNet learns a residual function:\n",
        "\n",
        "ð»\n",
        "(\n",
        "ð‘¥\n",
        ")\n",
        "=\n",
        "ð¹\n",
        "(\n",
        "ð‘¥\n",
        ")\n",
        "+\n",
        "ð‘¥\n",
        "H(x)=F(x)+x\n",
        "\n",
        "where:\n",
        "\n",
        "ð‘¥\n",
        "x = input to the residual block\n",
        "\n",
        "ð¹\n",
        "(\n",
        "ð‘¥\n",
        ")\n",
        "F(x) = output of stacked convolution layers (the â€œresidualâ€)\n",
        "\n",
        "ð¹\n",
        "(\n",
        "ð‘¥\n",
        ")\n",
        "+\n",
        "ð‘¥\n",
        "F(x)+x = final output of the block\n",
        "\n",
        "This is done by adding a skip (identity) connection that bypasses one or more layers.\n",
        "\n",
        "How They Address the Vanishing Gradient Problem\n",
        "The Problem in Deep CNNs\n",
        "\n",
        "As networks go deeper, gradients during backpropagation often vanish (become very small) or explode (become too large).\n",
        "\n",
        "This makes it extremely hard to train very deep networks (e.g., >30 layers).\n",
        "\n",
        "Without enough gradient flow, earlier layers donâ€™t update properly, leading to poor convergence.\n",
        "\n",
        "Residual Connections Solve This\n",
        "\n",
        "Direct Gradient Flow:\n",
        "\n",
        "The skip connection provides a shortcut path for gradients to flow directly back through the network.\n",
        "\n",
        "Even if convolution layers vanish/explode, the identity path ensures gradients donâ€™t disappear.\n",
        "\n",
        "Easier Optimization:\n",
        "\n",
        "Instead of forcing the network to learn\n",
        "ð»\n",
        "(\n",
        "ð‘¥\n",
        ")\n",
        "H(x) from scratch, it only learns the residual\n",
        "ð¹\n",
        "(\n",
        "ð‘¥\n",
        ")\n",
        "=\n",
        "ð»\n",
        "(\n",
        "ð‘¥\n",
        ")\n",
        "âˆ’\n",
        "ð‘¥\n",
        "F(x)=H(x)âˆ’x.\n",
        "\n",
        "Learning the difference (residual) is easier than learning the full mapping.\n",
        "\n",
        "Deeper Networks Become Trainable:\n",
        "\n",
        "ResNet models with 50, 101, 152 layers trained successfully, outperforming shallower ones.\n",
        "\n",
        "Before ResNet, training very deep CNNs (>20â€“30 layers) usually degraded accuracy due to vanishing gradients."
      ],
      "metadata": {
        "id": "R6zORL9Wr6sG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 6: Implement the LeNet-5 architectures using Tensorflow or PyTorch to\n",
        "classify the MNIST dataset. Report the accuracy and training time.\n",
        "(Include your Python code and output in the code box below.)'''\n",
        "# LeNet-5 Implementation on MNIST using PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Device configuration\n",
        "# -------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Data preprocessing (MNIST: 28x28 -> resize to 32x32)\n",
        "# -------------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Define LeNet-5 model\n",
        "# -------------------------------\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)      # 1x32x32 -> 6x28x28\n",
        "        self.pool1 = nn.AvgPool2d(2, stride=2)           # 6x28x28 -> 6x14x14\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)     # 6x14x14 -> 16x10x10\n",
        "        self.pool2 = nn.AvgPool2d(2, stride=2)           # 16x10x10 -> 16x5x5\n",
        "        self.conv3 = nn.Conv2d(16, 120, kernel_size=5)   # 16x5x5 -> 120x1x1\n",
        "        self.fc1 = nn.Linear(120, 84)\n",
        "        self.fc2 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.tanh(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = torch.tanh(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = torch.tanh(self.conv3(x))\n",
        "        x = x.view(-1, 120)\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = LeNet5().to(device)\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Loss and optimizer\n",
        "# -------------------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Training\n",
        "# -------------------------------\n",
        "num_epochs = 5\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Testing / Evaluation\n",
        "# -------------------------------\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"\\nTest Accuracy: {accuracy:.2f}%\")\n",
        "print(f\"Training Time: {training_time:.2f} seconds\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehrpIBe-sIXF",
        "outputId": "a2955408-0339-4e17-94bf-2a550fb3d47c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [00:00<00:00, 55.1MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 1.65MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:00<00:00, 14.3MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 7.21MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 0.2717\n",
            "Epoch [2/5], Loss: 0.0869\n",
            "Epoch [3/5], Loss: 0.0576\n",
            "Epoch [4/5], Loss: 0.0452\n",
            "Epoch [5/5], Loss: 0.0358\n",
            "\n",
            "Test Accuracy: 98.57%\n",
            "Training Time: 211.20 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transfer Learning with VGG16 on a Custom Dataset (Flowers/Animals)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import time\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Data Preprocessing\n",
        "# -------------------------------\n",
        "# Assume dataset structure:\n",
        "# dataset/\n",
        "#   train/\n",
        "#       class1/\n",
        "#       class2/\n",
        "#       ...\n",
        "#   val/\n",
        "#       class1/\n",
        "#       class2/\n",
        "\n",
        "train_dir = \"dataset/train\"\n",
        "val_dir = \"dataset/val\"\n",
        "img_size = (224, 224)  # VGG16 input size\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=32,\n",
        "    class_mode=\"categorical\"\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=32,\n",
        "    class_mode=\"categorical\"\n",
        ")\n",
        "\n",
        "num_classes = len(train_generator.class_indices)\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Load Pretrained VGG16 (without top)\n",
        "# -------------------------------\n",
        "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze base layers for feature extraction\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Add Custom Classifier\n",
        "# -------------------------------\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    Flatten(),\n",
        "    Dense(256, activation=\"relu\"),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Training\n",
        "# -------------------------------\n",
        "epochs = 5\n",
        "start_time = time.time()\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_generator\n",
        ")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Fine-tuning (optional: unfreeze last few layers of VGG16)\n",
        "# -------------------------------\n",
        "for layer in base_model.layers[-4:]:  # unfreeze last 4 conv layers\n",
        "    layer.trainable = True\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "history_finetune = model.fit(\n",
        "    train_generator,\n",
        "    epochs=3,\n",
        "    validation_data=val_generator\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Evaluation\n",
        "# -------------------------------\n",
        "val_loss, val_acc = model.evaluate(val_generator)\n",
        "print(f\"\\nValidation Accuracy: {val_acc*100:.2f}%\")\n",
        "print(f\"Training Time: {training_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "jHE2d1YBsPEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Visualizing Filters and Feature Maps of AlexNet\n",
        "\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------------------------\n",
        "# Load Pretrained AlexNet\n",
        "# ---------------------------\n",
        "alexnet = models.alexnet(pretrained=True)\n",
        "alexnet.eval()\n",
        "\n",
        "# ---------------------------\n",
        "# Load and Preprocess Image\n",
        "# ---------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load sample image (replace \"dog.jpg\" with your image file)\n",
        "img_path = \"dog.jpg\"\n",
        "img = Image.open(img_path).convert('RGB')\n",
        "input_tensor = transform(img).unsqueeze(0)  # add batch dimension\n",
        "\n",
        "# ---------------------------\n",
        "# Visualize Filters of 1st Conv Layer\n",
        "# ---------------------------\n",
        "filters = alexnet.features[0].weight.data.clone()\n",
        "\n",
        "fig, axes = plt.subplots(8, 8, figsize=(12, 12))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    if i < filters.shape[0]:\n",
        "        # Normalize filter for visualization\n",
        "        f = filters[i].clone()\n",
        "        f = (f - f.min()) / (f.max() - f.min())\n",
        "        ax.imshow(f.permute(1, 2, 0))\n",
        "    ax.axis('off')\n",
        "plt.suptitle(\"First Convolutional Layer Filters (AlexNet)\")\n",
        "plt.show()\n",
        "\n",
        "# ---------------------------\n",
        "# Extract Feature Maps\n",
        "# ---------------------------\n",
        "with torch.no_grad():\n",
        "    feature_maps = alexnet.features[0](input_tensor)\n",
        "\n",
        "# Visualize first 32 feature maps\n",
        "fig, axes = plt.subplots(4, 8, figsize=(15, 8))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    if i < feature_maps.shape[1]:\n",
        "        fmap = feature_maps[0, i].cpu().numpy()\n",
        "        ax.imshow(fmap, cmap='gray')\n",
        "    ax.axis('off')\n",
        "plt.suptitle(\"Feature Maps of First Conv Layer (AlexNet)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lLrrkR5Dt7wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Train GoogLeNet on CIFAR-10\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import models\n",
        "\n",
        "# ---------------------------\n",
        "# Device Configuration\n",
        "# ---------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ---------------------------\n",
        "# Data Preprocessing\n",
        "# ---------------------------\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize(224),  # GoogLeNet expects 224x224 input\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                         (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                         (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "# ---------------------------\n",
        "# Load GoogLeNet Model\n",
        "# ---------------------------\n",
        "net = models.googlenet(pretrained=False, aux_logits=True, num_classes=10)\n",
        "net = net.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# ---------------------------\n",
        "# Training Function\n",
        "# ---------------------------\n",
        "train_acc_list, val_acc_list = [], []\n",
        "\n",
        "def train_model(num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        correct, total, train_loss = 0, 0, 0\n",
        "\n",
        "        for images, labels in trainloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(images)\n",
        "\n",
        "            # GoogLeNet outputs tuple when aux_logits=True\n",
        "            if isinstance(outputs, tuple):\n",
        "                main_out, aux1_out, aux2_out = outputs\n",
        "                loss1 = criterion(main_out, labels)\n",
        "                loss2 = criterion(aux1_out, labels)\n",
        "                loss3 = criterion(aux2_out, labels)\n",
        "                loss = loss1 + 0.3 * (loss2 + loss3)\n",
        "                outputs = main_out\n",
        "            else:\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_acc = 100. * correct / total\n",
        "        train_acc_list.append(train_acc)\n",
        "\n",
        "        # Validation accuracy\n",
        "        net.eval()\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in testloader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = net(images)\n",
        "                if isinstance(outputs, tuple):\n",
        "                    outputs = outputs[0]\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        val_acc = 100. * correct / total\n",
        "        val_acc_list.append(val_acc)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "              f\"Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "# ---------------------------\n",
        "# Run Training\n",
        "# ---------------------------\n",
        "train_model(num_epochs=10)\n",
        "\n",
        "# ---------------------------\n",
        "# Plot Accuracy Curves\n",
        "# ---------------------------\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(train_acc_list, label='Training Accuracy')\n",
        "plt.plot(val_acc_list, label='Validation Accuracy')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.title(\"GoogLeNet on CIFAR-10\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HV98TylxsY4L"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working in a healthcare AI startup. Your team is tasked with\n",
        "developing a system that automatically classifies medical X-ray images into normal,\n",
        "pneumonia, and COVID-19. Due to limited labeled data, what approach would you\n",
        "suggest using among CNN architectures discussed (e.g., transfer learning with ResNet\n",
        "or Inception variants)? Justify your approach and outline a deployment strategy for\n",
        "production use.\n",
        "\n",
        "Ans:-\n",
        "\n",
        "Since the dataset is limited and medical imaging data is hard to annotate, the best approach would be:\n",
        "\n",
        "Transfer Learning using a pre-trained CNN (e.g., ResNet50 or Inception-v3) trained on ImageNet.\n",
        "\n",
        "Replace the final classification layer with a 3-class output layer (normal, pneumonia, COVID-19).\n",
        "\n",
        "Fine-tune only the higher layers (last few blocks) to adapt to medical image features while freezing the earlier convolutional layers (which capture generic features like edges, textures, and shapes).\n",
        "\n",
        "This helps in:\n",
        "\n",
        "Reducing computational cost (fewer trainable parameters).\n",
        "\n",
        "Overcoming limited data (leveraging learned representations).\n",
        "\n",
        "Improving accuracy and generalization compared to training from scratch.\n",
        "\n",
        "Justification\n",
        "\n",
        "ResNet (Residual Networks):\n",
        "\n",
        "Residual connections solve vanishing gradient problems â†’ allows deeper networks to converge.\n",
        "\n",
        "Works very well on X-ray images where subtle texture differences matter.\n",
        "\n",
        "Inception Variants (GoogLeNet, Inception-v3):\n",
        "\n",
        "Good at capturing multi-scale features.\n",
        "\n",
        "Can detect both local (small lesions) and global (lung shape) features.\n",
        "\n",
        "Among them, ResNet50 is widely used in medical imaging because of its balance between depth, accuracy, and computational efficiency.\n",
        "\n",
        "Deployment Strategy for Production\n",
        "\n",
        "Model Development\n",
        "\n",
        "Start with transfer learning on ResNet50 with ImageNet weights.\n",
        "\n",
        "Apply data augmentation (rotation, flipping, scaling) to artificially expand the dataset.\n",
        "\n",
        "Use early stopping, dropout, and L2 regularization to avoid overfitting.\n",
        "\n",
        "Evaluate using cross-validation to ensure robustness.\n",
        "\n",
        "Model Optimization\n",
        "\n",
        "Convert trained model to ONNX or TensorRT for optimized inference.\n",
        "\n",
        "Quantization or pruning can be used if deploying on resource-constrained devices (like hospital servers).\n",
        "\n",
        "Deployment\n",
        "\n",
        "Wrap the model into a REST API (Flask/FastAPI/Django).\n",
        "\n",
        "Deploy on cloud servers (AWS, GCP, Azure) for scalability.\n",
        "\n",
        "Ensure GPU acceleration if handling many requests in real time.\n",
        "\n",
        "Integration with Healthcare System\n",
        "\n",
        "Build a web dashboard for doctors â†’ upload X-ray â†’ get prediction + confidence scores.\n",
        "\n",
        "Provide explainability (Grad-CAM heatmaps) so doctors see which regions influenced the prediction.\n",
        "\n",
        "Post-Deployment Monitoring\n",
        "\n",
        "Continuously collect feedback from radiologists to refine the model.\n",
        "\n",
        "Retrain periodically with new labeled data.\n",
        "\n",
        "Maintain compliance with healthcare regulations (HIPAA, GDPR)."
      ],
      "metadata": {
        "id": "TZL207YEsokm"
      }
    }
  ]
}