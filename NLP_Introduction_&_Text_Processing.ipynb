{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Computational Linguistics and how does it relate to NLP?\n",
        "\n",
        "Ans:-\n",
        "Computational Linguistics is the scientific study of language using computational methods.\n",
        "It aims to understand how human language works by creating models that mimic or analyze linguistic behavior.\n",
        "\n",
        "It combines:\n",
        "\n",
        "Linguistics (syntax, semantics, phonetics, morphology, pragmatics)\n",
        "\n",
        "Computer Science (algorithms, data structures, machine learning)\n",
        "\n",
        "Mathematics (probability, statistics)\n",
        "\n",
        "The goal of CL\n",
        "\n",
        "To model natural language in a precise, testable way\n",
        "\n",
        "To build systems that understand linguistic rules\n",
        "\n",
        "To study language scientifically using computational tools\n",
        "\n",
        "CL is more theoretical, leaning toward understanding why language works the way it does.\n",
        "\n",
        "Computational Linguistics is the scientific study and modeling of language using computational methods, whereas NLP is the engineering discipline that applies these models to build language-based applications. They are deeply interconnected: CL provides theories and models, NLP uses them to create practical systems.\n",
        "\n",
        "Question 2: Briefly describe the historical evolution of Natural Language Processing\n",
        "\n",
        "Ans:-\n",
        "\n",
        "1. 1950s–1960s: Rule-Based Systems (Symbolic NLP)\n",
        "\n",
        "Early NLP relied on hand-crafted linguistic rules.\n",
        "\n",
        "Inspired by Noam Chomsky's grammar theories.\n",
        "\n",
        "Focus on machine translation (e.g., Georgetown-IBM experiment, 1954).\n",
        "\n",
        "Systems used simple pattern matching and manually written grammars.\n",
        "\n",
        "2. 1970s–1980s: Knowledge-Based and Linguistic Models\n",
        "\n",
        "Development of syntactic parsers, semantic networks, and expert systems.\n",
        "\n",
        "NLP systems started using world knowledge and linguistic theories.\n",
        "\n",
        "SHRDLU (1970) demonstrated natural language understanding in restricted domains.\n",
        "\n",
        "3. 1990s: Statistical NLP Revolution\n",
        "\n",
        "Shift from rules to probability and statistics.\n",
        "\n",
        "Introduction of:\n",
        "\n",
        "Hidden Markov Models (HMMs)\n",
        "\n",
        "N-grams\n",
        "\n",
        "Statistical Machine Translation\n",
        "\n",
        "Large corpora like the Brown Corpus emerged.\n",
        "\n",
        "Data-driven methods replaced hand-written rules.\n",
        "\n",
        "4. 2000s: Machine Learning Era\n",
        "\n",
        "Use of supervised and unsupervised learning for NLP tasks.\n",
        "\n",
        "Algorithms like SVMs, decision trees, and CRFs became common.\n",
        "\n",
        "Improvement in text classification, POS tagging, named entity recognition.\n",
        "\n",
        "5. 2010s: Deep Learning Era\n",
        "\n",
        "Major breakthroughs with neural networks:\n",
        "\n",
        "Word2Vec (2013)\n",
        "\n",
        "RNNs, LSTMs, GRUs\n",
        "\n",
        "Seq2Seq models with attention (2014)\n",
        "\n",
        "Enabled better machine translation, summarization, and sentiment analysis.\n",
        "\n",
        "6. 2017–Present: Transformer Models & Large Language Models\n",
        "\n",
        "Introduction of Transformers (Vaswani et al., 2017) changed NLP completely.\n",
        "\n",
        "Transformers enabled:\n",
        "\n",
        "BERT (2018)\n",
        "\n",
        "GPT series (2018–present)\n",
        "\n",
        "T5, XLNet, RoBERTa\n",
        "\n",
        "Large Language Models (LLMs) learn from billions of parameters and huge datasets.\n",
        "\n",
        "NLP now focuses on:\n",
        "\n",
        "Zero-shot learning\n",
        "\n",
        "Few-shot learning\n",
        "\n",
        "Multimodal models\n",
        "\n",
        "Generative AI (ChatGPT, Claude, Gemini)\n",
        "\n",
        "Question 3: List and explain three major use cases of NLP in today’s tech industry.\n",
        "\n",
        "Ans:-\n",
        "\n",
        "1. Chatbots and Virtual Assistants\n",
        "Use Case:\n",
        "\n",
        "Customer support chatbots\n",
        "\n",
        "Voice assistants like Siri, Alexa, Google Assistant\n",
        "\n",
        "AI support in banking, e-commerce, healthcare\n",
        "\n",
        "How NLP is used:\n",
        "\n",
        "Intent detection: Understand what the user wants\n",
        "\n",
        "Entity recognition: Extract important information (name, date, location)\n",
        "\n",
        "Natural language generation: Respond naturally and conversationally\n",
        "\n",
        "Impact:\n",
        "\n",
        "Reduces customer service cost\n",
        "\n",
        "Provides 24/7 automated assistance\n",
        "\n",
        "Improves user experience\n",
        "\n",
        "2. Sentiment Analysis\n",
        "Use Case:\n",
        "\n",
        "Analyzing customer reviews on Amazon, Flipkart\n",
        "\n",
        "Monitoring brand sentiment on social media\n",
        "\n",
        "Detecting public opinion for products, movies, political campaigns\n",
        "\n",
        "How NLP is used:\n",
        "\n",
        "Classifies text as positive, negative, or neutral\n",
        "\n",
        "Understands emotions, tone, and context\n",
        "\n",
        "Impact:\n",
        "\n",
        "Helps companies understand customer satisfaction\n",
        "\n",
        "Supports marketing and product decision-making\n",
        "\n",
        "Detects trends and public reactions\n",
        "\n",
        "3. Machine Translation\n",
        "Use Case:\n",
        "\n",
        "Google Translate, DeepL\n",
        "\n",
        "Real-time translation in apps, websites, and customer support\n",
        "\n",
        "Cross-language communication for global companies\n",
        "\n",
        "How NLP is used:\n",
        "\n",
        "Sequence-to-sequence models and Transformers\n",
        "\n",
        "Converts text from one language to another while preserving meaning\n",
        "\n",
        "Impact:\n",
        "\n",
        "Breaks language barriers\n",
        "\n",
        "Enables international business and communication\n",
        "\n",
        "Helps in multilingual content creation.\n",
        "\n",
        "Question 4: What is text normalization and why is it essential in text processing tasks?\n",
        "\n",
        "Ans:-\n",
        "\n",
        "Text normalization is the process of converting raw, unstructured text into a standard, consistent, and clean format so it can be easily processed by NLP models.\n",
        "\n",
        "It reduces variations in text caused by:\n",
        "\n",
        "Spelling differences\n",
        "\n",
        "Case differences\n",
        "\n",
        "Punctuation\n",
        "\n",
        "Abbreviations\n",
        "\n",
        "Slang or noisy data\n",
        "\n",
        " Why is Text Normalization Essential?\n",
        "\n",
        "It is essential because NLP models require uniform and clean input to perform accurately. Raw text contains a lot of noise (like uppercase letters, extra spaces, emojis, punctuation, slang), which can confuse models.\n",
        "\n",
        "Key reasons it is important:\n",
        "1. Improves Model Accuracy\n",
        "\n",
        "Removes inconsistencies that affect tokenization and feature extraction.\n",
        "\n",
        "Ensures similar words like “Dog”, “dog”, and “DOG” are treated as the same word.\n",
        "\n",
        "2. Reduces Data Sparsity\n",
        "\n",
        "Converts multiple forms of the same word (e.g., \"running\", \"runs\", \"ran\") into a standardized form.\n",
        "\n",
        "Helps statistical and ML models by reducing the vocabulary size.\n",
        "\n",
        "3. Enhances Performance of Downstream Tasks\n",
        "\n",
        "Tasks like sentiment analysis, translation, classification, and summarization work better with normalized text.\n",
        "\n",
        "Examples of Text Normalization Techniques\n",
        "\n",
        "Lowercasing\n",
        "\n",
        "Removing punctuation\n",
        "\n",
        "Stopword removal\n",
        "\n",
        "Stemming and Lemmatization\n",
        "\n",
        "Expanding contractions (e.g., “don’t” → “do not”)\n",
        "\n",
        "Removing special characters, emojis, URLs\n",
        "\n",
        "Spell correction\n",
        "\n",
        "Question 5: Compare and contrast stemming and lemmatization with suitable examples.\n",
        "\n",
        "Ans:-\n",
        "\n",
        "| Feature             | Stemming               | Lemmatization        |\n",
        "| ------------------- | ---------------------- | -------------------- |\n",
        "| Based on            | Heuristic rules        | Linguistic knowledge |\n",
        "| Accuracy            | Low–Medium             | High                 |\n",
        "| Output              | Not always a real word | Always a real word   |\n",
        "| Speed               | Faster                 | Slower               |\n",
        "| Context Awareness   | No                     | Yes                  |\n",
        "| Example (\"studies\") | \"studi\"                | \"study\"              |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "feuoCpKY5EQK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiTSIvCm5DbH",
        "outputId": "c1475f60-f143-4519-c5ed-89c3d774efdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Email Addresses:\n",
            "support@xyz.com\n",
            "hr@xyz.com\n",
            "john.doe@xyz.org\n",
            "jenny_clarke126@mail.co.us\n",
            "partners@xyz.biz\n"
          ]
        }
      ],
      "source": [
        "'''Question 6: Write a Python program that uses regular expressions (regex) to extract all\n",
        "email addresses from the following block of text:\n",
        "“Hello team, please contact us at support@xyz.com for technical issues, or reach out to\n",
        "our HR at hr@xyz.com. You can also connect with John at john.doe@xyz.org and jenny\n",
        "via jenny_clarke126@mail.co.us. For partnership inquiries, email partners@xyz.biz'''\n",
        "\n",
        "import re\n",
        "\n",
        "text = \"\"\"\n",
        "Hello team, please contact us at support@xyz.com for technical issues,\n",
        "or reach out to our HR at hr@xyz.com. You can also connect with John at\n",
        "john.doe@xyz.org and jenny via jenny_clarke126@mail.co.us.\n",
        "For partnership inquiries, email partners@xyz.biz\n",
        "\"\"\"\n",
        "\n",
        "# Regex pattern for email extraction\n",
        "pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "\n",
        "# Extract all emails\n",
        "emails = re.findall(pattern, text)\n",
        "\n",
        "print(\"Extracted Email Addresses:\")\n",
        "for email in emails:\n",
        "    print(email)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 7: Given the sample paragraph below, perform string tokenization and\n",
        "frequency distribution using Python and NLTK:\n",
        "“Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "in modern solutions is becoming increasingly critical.”'''\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "paragraph = \"\"\"\n",
        "Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "in modern solutions is becoming increasingly critical.\n",
        "\"\"\"\n",
        "\n",
        "# --------- Step 1: Tokenization ---------\n",
        "tokens = word_tokenize(paragraph)\n",
        "\n",
        "print(\"Tokens:\")\n",
        "print(tokens)\n",
        "\n",
        "# --------- Step 2: Frequency Distribution ---------\n",
        "freq_dist = FreqDist(tokens)\n",
        "\n",
        "print(\"\\nFrequency Distribution:\")\n",
        "for word, freq in freq_dist.items():\n",
        "    print(word, \":\", freq)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "JKbroMpQ6FYd",
        "outputId": "48b3db8d-ac8f-422e-d501-1461e85ed88e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2432422890.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# --------- Step 1: Tokenization ---------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tokens:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 8: Create a custom annotator using spaCy or NLTK that identifies and labels\n",
        "proper nouns in a given text.\n",
        "(Include your Python code and output in the code box below.)\n",
        "'''\n",
        "import spacy\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text\n",
        "text = \"\"\"\n",
        "Natural Language Processing (NLP) is widely used by companies like Google, Microsoft,\n",
        "and OpenAI. Researchers such as John Smith and Emily Clarke have contributed\n",
        "significantly to the field. The University of Cambridge and Stanford University\n",
        "are leaders in AI research.\n",
        "\"\"\"\n",
        "\n",
        "# Process text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Custom annotator to extract proper nouns\n",
        "proper_nouns = [(token.text, token.pos_) for token in doc if token.pos_ == \"PROPN\"]\n",
        "\n",
        "print(\"Proper Noun Annotations:\")\n",
        "for word, label in proper_nouns:\n",
        "    print(f\"{word} --> {label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JdPzc-_6LF1",
        "outputId": "dbad746e-ad03-40fb-d415-27a4a2ad9aa6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proper Noun Annotations:\n",
            "Natural --> PROPN\n",
            "Language --> PROPN\n",
            "Processing --> PROPN\n",
            "NLP --> PROPN\n",
            "Google --> PROPN\n",
            "Microsoft --> PROPN\n",
            "OpenAI --> PROPN\n",
            "John --> PROPN\n",
            "Smith --> PROPN\n",
            "Emily --> PROPN\n",
            "Clarke --> PROPN\n",
            "University --> PROPN\n",
            "Cambridge --> PROPN\n",
            "Stanford --> PROPN\n",
            "University --> PROPN\n",
            "AI --> PROPN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 9: Using Genism, demonstrate how to train a simple Word2Vec model on the\n",
        "following dataset consisting of example sentences:\n",
        "dataset = [\n",
        " \"Natural language processing enables computers to understand human language\",\n",
        " \"Word embeddings are a type of word representation that allows words with similar\n",
        "meaning to have similar representation\",\n",
        " \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        " \"Text preprocessing is a critical step before training word embeddings\",\n",
        " \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "Write code that tokenizes the dataset, preprocesses it, and trains a Word2Vec model using\n",
        "Gensim.\n",
        "'''\n",
        "# Install gensim if needed\n",
        "# !pip install gensim\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# -------------------------\n",
        "# Dataset\n",
        "# -------------------------\n",
        "dataset = [\n",
        "    \"Natural language processing enables computers to understand human language\",\n",
        "    \"Word embeddings are a type of word representation that allows words with similar meaning to have similar representation\",\n",
        "    \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        "    \"Text preprocessing is a critical step before training word embeddings\",\n",
        "    \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "\n",
        "# -------------------------\n",
        "# Step 1: Preprocessing & Tokenization\n",
        "# -------------------------\n",
        "# simple_preprocess() lowers, tokenizes, removes punctuation and short words\n",
        "tokenized_data = [simple_preprocess(sentence) for sentence in dataset]\n",
        "\n",
        "print(\"Tokenized Sentences:\")\n",
        "for tokens in tokenized_data:\n",
        "    print(tokens)\n",
        "\n",
        "# -------------------------\n",
        "# Step 2: Train Word2Vec Model\n",
        "# -------------------------\n",
        "model = Word2Vec(\n",
        "    sentences=tokenized_data,\n",
        "    vector_size=50,      # size of word vector\n",
        "    window=3,            # context window size\n",
        "    min_count=1,         # include all words\n",
        "    workers=4,           # parallel threads\n",
        "    sg=1                 # skip-gram (sg=1), CBOW (sg=0)\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Step 3: Example Output\n",
        "# -------------------------\n",
        "print(\"\\nVector for word 'language':\")\n",
        "print(model.wv['language'])\n",
        "\n",
        "print(\"\\nMost similar words to 'word':\")\n",
        "print(model.wv.most_similar('word'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "OH5AYhVY6VW2",
        "outputId": "73ed9098-314f-49c1-c222-d934fbbcba29"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gensim'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1739082483.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# !pip install gensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msimple_preprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you are a data scientist at a fintech startup. You’ve been tasked\n",
        "with analyzing customer feedback. Outline the steps you would take to clean, process,\n",
        "and extract useful insights using NLP techniques from thousands of customer reviews.\n",
        "\n",
        "Ans:-\n",
        "\n",
        "1. Data Collection\n",
        "\n",
        "Gather reviews from:\n",
        "\n",
        "Mobile app feedback\n",
        "\n",
        "Play Store / App Store reviews\n",
        "\n",
        "Website feedback forms\n",
        "\n",
        "Customer support chat logs\n",
        "\n",
        "Store data in a database or CSV for preprocessing.\n",
        "\n",
        "2. Data Cleaning\n",
        "\n",
        "Clean the raw text to remove noise:\n",
        "\n",
        "a. Remove unwanted characters\n",
        "\n",
        "HTML tags\n",
        "\n",
        "URLs\n",
        "\n",
        "Emojis\n",
        "\n",
        "Special symbols\n",
        "\n",
        "b. Normalize text\n",
        "\n",
        "Lowercasing\n",
        "\n",
        "Expanding contractions (can’t → cannot)\n",
        "\n",
        "Spell correction (opt-in)\n",
        "\n",
        "c. Remove stopwords\n",
        "\n",
        "Words like “the”, “is”, “and” add no meaning.\n",
        "\n",
        "d. Tokenization\n",
        "\n",
        "Split sentences into words for processing.\n",
        "\n",
        "3. Text Preprocessing\n",
        "a. Lemmatization or Stemming\n",
        "\n",
        "Convert words to their root form\n",
        "e.g., “running” → “run”\n",
        "\n",
        "b. Handle negations\n",
        "\n",
        "Combine negation + word\n",
        "e.g., “not good” → “not_good”\n",
        "\n",
        "c. Remove very rare/very frequent terms\n",
        "\n",
        "Reduces noise for machine learning models.\n",
        "\n",
        "4. Exploratory Data Analysis (EDA)\n",
        "a. Word frequency analysis\n",
        "\n",
        "Create frequency distribution of important terms\n",
        "\n",
        "Identify common complaints and positive feedback\n",
        "\n",
        "b. N-gram analysis\n",
        "\n",
        "Bigram/trigram extraction to find patterns like:\n",
        "\n",
        "“late payment”\n",
        "\n",
        "“poor customer support”\n",
        "\n",
        "“fast transactions”\n",
        "\n",
        "c. Word Cloud\n",
        "\n",
        "Visualize the most common words (after filtering).\n",
        "\n",
        "5. Sentiment Analysis\n",
        "Approaches:\n",
        "\n",
        "Rule-based (e.g., VADER for short reviews)\n",
        "\n",
        "Machine Learning (SVM, Naive Bayes)\n",
        "\n",
        "Deep Learning (LSTM, BERT, RoBERTa)\n",
        "\n",
        "Output:\n",
        "\n",
        "Positive\n",
        "\n",
        "Negative\n",
        "\n",
        "Neutral\n",
        "\n",
        "Helps identify how customers feel about:\n",
        "\n",
        "App performance\n",
        "\n",
        "Financial services\n",
        "\n",
        "Onboarding process\n",
        "\n",
        "6. Topic Modeling\n",
        "\n",
        "Use unsupervised learning to discover hidden themes:\n",
        "\n",
        "Methods:\n",
        "\n",
        "LDA (Latent Dirichlet Allocation)\n",
        "\n",
        "NMF (Non-negative Matrix Factorization)\n",
        "\n",
        "BERTopic (Transformer-based, highly effective)\n",
        "\n",
        "Typical fintech topics:\n",
        "\n",
        "App crashes\n",
        "\n",
        "Payment delays\n",
        "\n",
        "Loan approval issues\n",
        "\n",
        "Security concerns\n",
        "\n",
        "Good customer service\n",
        "\n",
        "7. Aspect-Based Sentiment Analysis (ABSA)\n",
        "\n",
        "Break down reviews by aspects:\n",
        "\n",
        "Aspect\tSentiment\n",
        "App UI\tPositive\n",
        "KYC verification\tNegative\n",
        "Transaction speed\tMixed\n",
        "Customer support\tNegative\n",
        "\n",
        "Helps identify exactly which features need improvement.\n",
        "\n",
        "8. Classification (Optional)\n",
        "\n",
        "Build models to auto-classify reviews into categories:\n",
        "\n",
        "Bug report\n",
        "\n",
        "Feature request\n",
        "\n",
        "Complaint\n",
        "\n",
        "Praise\n",
        "\n",
        "Algorithms: Logistic Regression, SVM, Random Forest, or BERT-based classifiers.\n",
        "\n",
        "9. Summarization\n",
        "\n",
        "Use text summarization models (e.g., T5, BART) to generate:\n",
        "\n",
        "Monthly summary of customer issues\n",
        "\n",
        "Executive reports\n",
        "\n",
        "10. Visualization & Reporting\n",
        "\n",
        "Create dashboards using:\n",
        "\n",
        "Power BI, Tableau\n",
        "\n",
        "Matplotlib / Plotly\n",
        "\n",
        "Metrics to include:\n",
        "\n",
        "Sentiment trends over time\n",
        "\n",
        "Word frequency\n",
        "\n",
        "Most common complaint categories\n",
        "\n",
        "Satisfaction score\n",
        "\n",
        "11. Actionable Insights for Product Team\n",
        "\n",
        "From NLP analysis, provide recommendations such as:\n",
        "\n",
        "Improve KYC verification time\n",
        "\n",
        "Fix payment gateway reliability\n",
        "\n",
        "Simplify loan application flow\n",
        "\n",
        "Enhance customer support responsiveness"
      ],
      "metadata": {
        "id": "nPKD29z-6fUx"
      }
    }
  ]
}